# -*- coding: utf-8 -*-
"""CS5720_HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFbLKIXslDEzYTb-kAIvABNpiTGFTCUj
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import os

# Task 1: Tensor Transformations & Reshaping
# Step 1: Generate a tensor with dimensions (4, 6)
tensor_data = tf.random.uniform((4, 6))
print("Generated Tensor:", tensor_data.numpy())

# Step 2: Determine tensor rank and shape
tensor_rank = tf.rank(tensor_data).numpy()
tensor_shape = tensor_data.shape
print(f"Rank: {tensor_rank}, Shape: {tensor_shape}")

# Step 3: Modify shape and transpose
reshaped_data = tf.reshape(tensor_data, (2, 3, 4))
transposed_data = tf.transpose(reshaped_data, perm=[1, 0, 2])
print("Reshaped Tensor:", reshaped_data.numpy())
print("Transposed Tensor:", transposed_data.numpy())

# Step 4: Broadcasting & Summation
small_data = tf.random.uniform((1, 4))
broadcasted_data = tf.broadcast_to(small_data, (4, 4))
result_data = tensor_data[:, :4] + broadcasted_data
print("Broadcasted Tensor:", broadcasted_data.numpy())
print("Summed Tensor:", result_data.numpy())

# Explanation of Broadcasting Concept
print("Broadcasting enables a smaller tensor to be extended to fit a larger tensor's shape for element-wise operations.")



# Task 2: Compute and Compare Loss Functions
y_actual = tf.constant([0.0, 1.0, 1.0, 0.0])
y_predicted = tf.constant([0.2, 0.9, 0.8, 0.1])

mse_loss_fn = MeanSquaredError()
cce_loss_fn = CategoricalCrossentropy()

mse_result = mse_loss_fn(y_actual, y_predicted).numpy()
cce_result = cce_loss_fn(tf.expand_dims(y_actual, axis=0), tf.expand_dims(y_predicted, axis=0)).numpy()
print(f"MSE Loss: {mse_result}, CCE Loss: {cce_result}")

# Alter predictions and compute loss again
y_predicted_updated = tf.constant([0.1, 0.8, 0.9, 0.2])
mse_updated = mse_loss_fn(y_actual, y_predicted_updated).numpy()
cce_updated = cce_loss_fn(tf.expand_dims(y_actual, axis=0), tf.expand_dims(y_predicted_updated, axis=0)).numpy()
print(f"Updated MSE Loss: {mse_updated}, Updated CCE Loss: {cce_updated}")

# Visualize Loss Functions
plt.bar(["MSE", "CCE"], [mse_result, cce_result], color=['blue', 'red'])
plt.xlabel("Loss Type")
plt.ylabel("Loss Value")
plt.title("MSE vs Cross-Entropy Loss Comparison")
plt.show()

# Task 3: Neural Network Training with TensorBoard

# Define the build_model function
def build_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)), # Assuming MNIST image size 28x28
        Dense(128, activation='relu'),
        Dense(10, activation='softmax') # Assuming 10 classes for output
    ])
    return model

log_directory = "logs/fit/"
os.makedirs(log_directory, exist_ok=True)

# Load MNIST data (Assuming this is intended for training)
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the pixel values
train_images = train_images / 255.0
test_images = test_images / 255.0


model_tb = build_model()
model_tb.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_directory, histogram_freq=1)
model_tb.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels), callbacks=[tb_callback])

# Launch TensorBoard
print("To launch TensorBoard, use: tensorboard --logdir logs/fit/")

# Answering Key Questions
print("1. Observing accuracy curves: Adam converges quicker, whereas SGD takes longer but may generalize better.")
print("2. Detecting overfitting using TensorBoard: A widening gap between training and validation accuracy signals overfitting.")
print("3. Effect of increased epochs: More training can enhance accuracy initially but might lead to overfitting.")